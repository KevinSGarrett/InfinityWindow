2. New features you proposed – how to fit them into this project

I’ll go feature-by-feature and give you a concrete implementation shape:
DB changes → backend endpoints → UI changes → AI integration (where relevant).

2.1 Name conversations & group them into folders

You already have:

type Conversation = {
  id: number;
  project_id: number;
  title?: string | null;
};

a) Naming conversations

Backend:

Add a small endpoint:

PATCH /conversations/{id} with body { "title": "New title" }

In DB, you already have a title column, so this might be just wiring the route.

Frontend:

In the left Conversations list:

Show the title if present, fallback to Chat conversation #X (you already do this).

Add a simple inline edit:

When you hover or click a pencil icon, show an <input> with the current title.

On blur or Enter, call PATCH /conversations/{id} and update local state.

This gives you “ChatGPT-style” renaming with minimal effort.

b) Folders for conversations

DB:

New table ConversationFolder:

id, project_id, name

Add folder_id (nullable) to Conversation.

Backend:

Endpoints:

GET /projects/{id}/conversation_folders

POST /conversation_folders (create)

PATCH /conversation_folders/{id} (rename)

PATCH /conversations/{id}/move with { folder_id: number | null }

Frontend:

In the left column:

Above the conversation list, show folders:

Either as collapsible sections: Folder 1, Folder 2, “No folder”.

Drag & drop or dropdown (“Move to folder…”) for each conversation.

That gets you “ChatGPT with folders” style organization.

2.2 Project-wide TODO list panel (AI-assisted)

You described exactly why this is needed: long projects, pivots, changes, and you lose the thread. Let’s give it a first-class structure.

a) Add a Task model

DB:

Task table fields (starter set):

id

project_id

conversation_id (nullable, link to where it came up)

title (short label)

description (longer notes)

status ("open" | "in_progress" | "done")

priority (optional: "low" | "normal" | "high")

created_at, updated_at

b) Backend API

GET /projects/{id}/tasks – list tasks for a project

POST /tasks – create a task

PATCH /tasks/{id} – update status/title/description

(Optional) DELETE /tasks/{id} – remove obsolete tasks

c) UI: right-hand side “Tasks” tab

Right column could get two tabs:

Memory (what you have now: search & docs)

Tasks

On Tasks tab:

Show a list grouped by status:

Open / In Progress / Done

Each task has:

Checkbox/toggle for status

Title (click to expand description)

Maybe a subtle “From conversation #23” link

Add a simple “+ New task” at the top.

d) AI-assisted updating (how to do it sanely)

You could try to auto-update on every message, but that can get chaotic. A safer v1:

Backend function update_tasks_from_conversation(project_id, conversation_id):

Pull recent N messages from that conversation.

Fetch existing tasks for that project.

Call a cheap model (e.g. your budget mode) with a prompt like:

“Here is the conversation so far and the existing task list.
Propose a JSON patch: tasks to add, tasks to mark done, tasks to rename/clarify.”

Apply that patch to the DB.

Expose it via:

Button in the UI: “Update tasks from this conversation”.

Later, if it works well, you can:

Hook this into /chat: after each assistant reply, call update_tasks_from_conversation automatically for that project.

Show a small “Tasks updated” toast when it changes things.

This gives you exactly what you want: a continuously updated project TODO list without you having to manually reconstruct the outline every time.

2.3 Extra organization features I’d suggest

Given your goals (long-running projects), I’d add these later:

Project “charter” / instruction page

Per project: a rich-text “This project is about X; here are the constraints and goals.”

Always fed into the system prompt for that project.

Decision log

Table of “We decided to use X not Y because Z” entries.

Assistant can reference it to avoid reopening old debates.

Milestones / phases

Group tasks under named milestones (Phase 1, Phase 2, etc.).

Nice mental model for “where am I in this big project.”

Those all live nicely next to your TODO panel.

2.4 Cost / tokens / model usage panel

You want:

Per-message:

Which model

Tokens in/out

Cost

Per-conversation totals.

a) Backend: capture usage

In generate_reply_from_history, after calling OpenAI:

Read response.usage (if present):

prompt_tokens

completion_tokens

total_tokens

Add a MessageUsage or extend Message:

model

prompt_tokens

completion_tokens

total_tokens

cost_usd (approx)

To calculate cost:

Keep a small mapping in code:

PRICE_TABLE = {
    "gpt-5.1": {"input": 0.000X, "output": 0.000Y},
    ...
}


cost = prompt_tokens * input_price + completion_tokens * output_price.

(You can adjust these numbers as you like; they don’t need to be exact to the cent.)

Add endpoints:

GET /conversations/{id}/usage → returns:

per-message usage

aggregate totals for the conversation

b) UI: small “Usage & cost” box

In the middle column, under the chat (or at the very bottom of the page):

Show a compact box:

Conversation stats
- Model(s) used: gpt-5.1, gpt-5-nano, ...
- Total messages: 42
- Total tokens: 12,345 (in) / 8,765 (out)
- Approx cost: $0.XX
Last reply
- Model: gpt-5.1
- Tokens: 1,234 in / 567 out
- Cost: $0.00X


It stays out of the way but is always there when you scroll to the bottom.

2.5 Give the AI real access to your local project directory

You already ingest from C:\InfinityWindow. Now you want:

InfinityWindow to know that this is “the project root” for a given project.

Ability to browse, read, and update files directly.

a) Tell the backend what the local root is

Add a field to the Project model:

local_root_path (nullable string)

Add endpoints:

PATCH /projects/{id} with { "local_root_path": "C:\\InfinityWindow" }

GET /projects/{id} (already exists; just include local_root_path in the JSON).

UI:

In the left “Project” selector area:

Add a small input: “Local project path” + Save button.

Store it per project.

b) File browser APIs

Backend endpoints (all must restrict access to stay inside local_root_path):

GET /projects/{id}/files?path=subdir/optional
→ returns list: { name, type: "file" | "dir" }.

GET /projects/{id}/file?path=relative/path.py
→ returns { content: "…file text…" }.

PUT /projects/{id}/file with body { path, content }
→ writes the file to disk.

⚠️ Important safety things:

Normalize paths and ensure you don’t escape the root:

Reject anything where real_path is outside local_root_path.

Consider automatically doing a git diff or backup before overwriting.

c) UI: File explorer panel

Add another tab on the right (“Files”):

Show tree or just a simple folder view:

Click folder → load its children.

Click file → open read-only view + “Open for editing” toggle.

Later, this will tie into the “AI editing files for you” feature.

2.6 Let the system update local files for you

Goal: No more copy/paste full files from the chat.

Given the file API above, there are two levels:

Level 1 – Manual “Apply this change”

Assistant responds with code and metadata like:

“Update backend/app/llm/openai_client.py with the following content: …”

The UI detects a pattern (e.g. a fenced code block with a header // FILE: path).

You click an “Apply to file” button under that block.

The frontend calls:

PUT /projects/{id}/file with { path, content }.

This is already a massive quality of life improvement with you still in charge of when changes are applied.

Level 2 – Semi-automatic patching

More advanced:

Backend function:

POST /projects/{id}/file/patch with { path, old_snippet, new_snippet }.

System:

Reads the current file.

Confirms old_snippet still exists.

Replaces it with new_snippet.

Returns a mini diff for the UI to show.

UI flow:

Assistant replies with:

“Replace this old block: … with this new block: …”

UI sends the patch to backend.

Shows diff; you click “Apply”.

You still stay in the loop, but you’re not manually editing files.

2.7 Terminal window the system can use

This is the spiciest feature, because letting an AI run shell commands is powerful and potentially dangerous. But on your local machine, with guardrails, it’s doable.

a) Backend: command runner

Endpoint (simplest version):

POST /terminal/run
Body: { project_id, shell: "powershell" | "bash", command: "..." }

Backend logic:

Resolve working directory from project’s local_root_path.

Run the command with subprocess:

Set timeout (e.g. 30s).

Capture stdout and stderr.

Return { stdout, stderr, exit_code }.

Safety tips:

Start with manual use only (you, the human, type the command).

If you ever let the LLM invoke it automatically, add a confirm step:

AI suggests: pip install ...

UI shows: “Run this command?” [Yes/No].

b) UI: terminal panel

Add a bottom drawer/panel: “Terminal”

Text input for command.

Dropdown for shell type (default from OS).

Output area showing the last run’s stdout/stderr.

Later, if you want AI-driven terminal use:

Add a “tool” in the backend: run_terminal_command.

When the AI wants to run something, it returns a structured tool call.

Backend asks the UI to confirm.

After running, the output is fed back into the conversation.

That gives you the “no more copy-paste between terminal and chat” loop.

2.8 UI redesign to be production-grade

You’re absolutely right: the current layout is functional but cramped and text-heavy.

I’d restructure like this:

Layout proposal

Left sidebar (fixed width)

Project selector

Conversation tree (folders + conversations)

Center column (flex, main area)

Chat header (project name, mode selector, maybe quick model badge)

Messages (scrollable, full height)

Input at bottom (sticky)

Right sidebar (fixed width, tabs)
Tabs:

Docs – current project docs list + search

Tasks – TODOs & milestones

Files – file explorer

Bottom bar (full width or under center)

Collapsible “Terminal” drawer

Tiny “Usage & cost” panel

Technical tweaks

Use a full viewport layout:

Root <div> with height: 100vh; display: flex;

Columns with flex: 0 0 260px (left), flex: 1 (center), flex: 0 0 320px (right).

Inside each column, use display: flex; flex-direction: column; and overflow: hidden; so inner sections can scroll independently:

Conversation list → overflow-y: auto;

Chat messages → overflow-y: auto;

Docs/Tasks lists → overflow-y: auto;

Design-wise, you can:

Stick with pure CSS but:

Add clean spacing, borders, and consistent typography.

Or bring in something like Tailwind or a component library later to polish visuals.

3. So… what should we actually do next?

Given everything you’ve got, and everything you want, I’d suggest this order:

Conversation quality-of-life

Naming conversations

Optional folders

Project TODO panel (tasks model + right-side Tasks tab)

Manual tasks first

Then add “Update tasks from conversation” AI button

Usage & cost tracking panel

Log usage in backend

Add small summary box in UI

Local project root + file APIs

Set local_root_path per project

Implement list/read/write endpoints

Add Files tab in UI

Manual “Apply change to file” from AI response

Click-to-apply for generated code changes

Terminal panel (manual use first)

Only after file handling feels solid

UI visual redesign / refactor with all panels in mind