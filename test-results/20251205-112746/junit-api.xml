<?xml version="1.0" encoding="utf-8"?><testsuites name="pytest tests"><testsuite name="pytest" errors="1" failures="5" skipped="0" tests="9" time="28.333" timestamp="2025-12-05T11:27:54.961877-06:00" hostname="DESKTOP-CJV735V"><testcase classname="test_api_projects" name="test_project_validation_requires_name" time="0.314"><failure message="assert 200 == 422&#10; +  where 200 = &lt;Response [200 OK]&gt;.status_code">client = &lt;starlette.testclient.TestClient object at 0x000001C08416EF60&gt;

    def test_project_validation_requires_name(client):
        """A-Proj-02: Reject empty project names."""
        resp = client.post(
            "/projects",
            json={
                "name": "",
                "description": "invalid",
                "local_root_path": os.getcwd(),
            },
        )
&gt;       assert resp.status_code == 422
E       assert 200 == 422
E        +  where 200 = &lt;Response [200 OK]&gt;.status_code

C:\InfinityWindow_QA\qa\tests_api\test_api_projects.py:36: AssertionError</failure></testcase><testcase classname="test_ingestion_e2e" name="test_basic_ingestion_happy_path" time="8.094" /><testcase classname="test_api_projects" name="test_create_list_update_project" time="0.317"><failure message="assert 405 == 200&#10; +  where 405 = &lt;Response [405 Method Not Allowed]&gt;.status_code">client = &lt;starlette.testclient.TestClient object at 0x00000297BE2CF230&gt;
project = {'description': 'QA autogenerated project', 'id': 1, 'instruction_text': None, 'instruction_updated_at': None, ...}

    def test_create_list_update_project(client, project):
        """A-Proj-01: Create project, list it, update metadata."""
        list_resp = client.get("/projects")
        assert list_resp.status_code == 200
        assert any(p["id"] == project["id"] for p in list_resp.json())
    
        update = client.patch(
            f"/projects/{project['id']}",
            json={"description": "Updated by QA", "pinned_note_text": "QA note"},
        )
        assert update.status_code == 200
        detail = client.get(f"/projects/{project['id']}")
&gt;       assert detail.status_code == 200
E       assert 405 == 200
E        +  where 405 = &lt;Response [405 Method Not Allowed]&gt;.status_code

C:\InfinityWindow_QA\qa\tests_api\test_api_projects.py:20: AssertionError</failure></testcase><testcase classname="test_api_chat_routing" name="test_chat_modes_stubbed" time="4.403"><failure message="AttributeError: 'NoneType' object has no attribute 'chat'">client = &lt;starlette.testclient.TestClient object at 0x000001FD040731D0&gt;
project = {'description': 'QA autogenerated project', 'id': 2, 'instruction_text': None, 'instruction_updated_at': None, ...}

    def test_chat_modes_stubbed(client, project):
        """C-Chat-02: Chat returns stubbed replies for each mode."""
        for mode in ["auto", "fast", "deep", "budget", "research", "code"]:
&gt;           resp = client.post(
                "/chat",
                json={
                    "project_id": project["id"],
                    "mode": mode,
                    "message": f"ping {mode}",
                },
            )

C:\InfinityWindow_QA\qa\tests_api\test_api_chat_routing.py:9: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
C:\InfinityWindow_QA\.venv\Lib\site-packages\starlette\testclient.py:546: in post
    return super().post(
C:\InfinityWindow_QA\.venv\Lib\site-packages\httpx\_client.py:1144: in post
    return self.request(
C:\InfinityWindow_QA\.venv\Lib\site-packages\starlette\testclient.py:445: in request
    return super().request(
C:\InfinityWindow_QA\.venv\Lib\site-packages\httpx\_client.py:825: in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\InfinityWindow_QA\.venv\Lib\site-packages\httpx\_client.py:914: in send
    response = self._send_handling_auth(
C:\InfinityWindow_QA\.venv\Lib\site-packages\httpx\_client.py:942: in _send_handling_auth
    response = self._send_handling_redirects(
C:\InfinityWindow_QA\.venv\Lib\site-packages\httpx\_client.py:979: in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\InfinityWindow_QA\.venv\Lib\site-packages\httpx\_client.py:1014: in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\InfinityWindow_QA\.venv\Lib\site-packages\starlette\testclient.py:348: in handle_request
    raise exc
C:\InfinityWindow_QA\.venv\Lib\site-packages\starlette\testclient.py:345: in handle_request
    portal.call(self.app, scope, receive, send)
C:\InfinityWindow_QA\.venv\Lib\site-packages\anyio\from_thread.py:326: in call
    return cast(T_Retval, self.start_task_soon(func, *args).result())
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\Windows 11\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\_base.py:456: in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
C:\Users\Windows 11\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\_base.py:401: in __get_result
    raise self._exception
C:\InfinityWindow_QA\.venv\Lib\site-packages\anyio\from_thread.py:257: in _call_func
    retval = await retval_or_awaitable
             ^^^^^^^^^^^^^^^^^^^^^^^^^
C:\InfinityWindow_QA\.venv\Lib\site-packages\fastapi\applications.py:1139: in __call__
    await super().__call__(scope, receive, send)
C:\InfinityWindow_QA\.venv\Lib\site-packages\starlette\applications.py:107: in __call__
    await self.middleware_stack(scope, receive, send)
C:\InfinityWindow_QA\.venv\Lib\site-packages\starlette\middleware\errors.py:186: in __call__
    raise exc
C:\InfinityWindow_QA\.venv\Lib\site-packages\starlette\middleware\errors.py:164: in __call__
    await self.app(scope, receive, _send)
C:\InfinityWindow_QA\.venv\Lib\site-packages\starlette\middleware\cors.py:85: in __call__
    await self.app(scope, receive, send)
C:\InfinityWindow_QA\.venv\Lib\site-packages\starlette\middleware\exceptions.py:63: in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
C:\InfinityWindow_QA\.venv\Lib\site-packages\starlette\_exception_handler.py:53: in wrapped_app
    raise exc
C:\InfinityWindow_QA\.venv\Lib\site-packages\starlette\_exception_handler.py:42: in wrapped_app
    await app(scope, receive, sender)
C:\InfinityWindow_QA\.venv\Lib\site-packages\fastapi\middleware\asyncexitstack.py:18: in __call__
    await self.app(scope, receive, send)
C:\InfinityWindow_QA\.venv\Lib\site-packages\starlette\routing.py:716: in __call__
    await self.middleware_stack(scope, receive, send)
C:\InfinityWindow_QA\.venv\Lib\site-packages\starlette\routing.py:736: in app
    await route.handle(scope, receive, send)
C:\InfinityWindow_QA\.venv\Lib\site-packages\starlette\routing.py:290: in handle
    await self.app(scope, receive, send)
C:\InfinityWindow_QA\.venv\Lib\site-packages\fastapi\routing.py:119: in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
C:\InfinityWindow_QA\.venv\Lib\site-packages\starlette\_exception_handler.py:53: in wrapped_app
    raise exc
C:\InfinityWindow_QA\.venv\Lib\site-packages\starlette\_exception_handler.py:42: in wrapped_app
    await app(scope, receive, sender)
C:\InfinityWindow_QA\.venv\Lib\site-packages\fastapi\routing.py:105: in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
C:\InfinityWindow_QA\.venv\Lib\site-packages\fastapi\routing.py:385: in app
    raw_response = await run_endpoint_function(
C:\InfinityWindow_QA\.venv\Lib\site-packages\fastapi\routing.py:286: in run_endpoint_function
    return await run_in_threadpool(dependant.call, **values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\InfinityWindow_QA\.venv\Lib\site-packages\starlette\concurrency.py:32: in run_in_threadpool
    return await anyio.to_thread.run_sync(func)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\InfinityWindow_QA\.venv\Lib\site-packages\anyio\to_thread.py:61: in run_sync
    return await get_async_backend().run_sync_in_worker_thread(
C:\InfinityWindow_QA\.venv\Lib\site-packages\anyio\_backends\_asyncio.py:2525: in run_sync_in_worker_thread
    return await future
           ^^^^^^^^^^^^
C:\InfinityWindow_QA\.venv\Lib\site-packages\anyio\_backends\_asyncio.py:986: in run
    result = context.run(func, *args)
             ^^^^^^^^^^^^^^^^^^^^^^^^
C:\InfinityWindow_QA\backend\app\api\main.py:3526: in chat
    raw_reply_text = generate_reply_from_history(
C:\InfinityWindow_QA\backend\app\llm\openai_client.py:527: in generate_reply_from_history
    raise last_error
C:\InfinityWindow_QA\backend\app\llm\openai_client.py:505: in generate_reply_from_history
    result = _call_model(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

messages = [{'content': "You are InfinityWindow, a helpful AI assistant. You work inside a long-lived project workspace with pers...commands and then interpret the results you are given.\n", 'role': 'system'}, {'content': 'ping auto', 'role': 'user'}]
model = 'gpt-4.1', temperature = 0.4, max_output_tokens = None, usage_out = {}

    def _call_model(
        messages: List[Dict[str, str]],
        model: str,
        temperature: Optional[float] = None,
        max_output_tokens: Optional[int] = None,
        usage_out: Optional[Dict[str, Any]] = None,
    ) -&gt; str:
        """
        Low‑level helper that actually calls OpenAI.
    
        - Uses Responses API for modern models (gpt‑5.*, o3.*, o4.*, gpt‑4o*).
        - Uses Chat Completions API for older chat models (gpt‑4.1, gpt‑4.1‑nano, 3.5, etc.).
        - IMPORTANT: we DO NOT send 'temperature' to the Responses API
          (models like gpt‑5‑nano reject it with a 400 error).
        """
        client = get_client()
        use_responses = _is_responses_model(model)
    
        print(
            f"[LLM] Using OpenAI model '{model}' via "
            f"{'Responses API' if use_responses else 'Chat Completions'}"
        )
    
        # We'll fill these from resp.usage when available
        tokens_in = 0
        tokens_out = 0
        total_tokens = 0
    
        if use_responses:
            # Responses API: do NOT include temperature (some models don't support it).
            kwargs: Dict[str, object] = {
                "model": model,
                "input": messages,
            }
            if max_output_tokens is not None:
                kwargs["max_output_tokens"] = max_output_tokens
    
            resp = client.responses.create(**kwargs)
            text = _extract_text_from_responses(resp)
    
            # Try to read usage.input_tokens / usage.output_tokens / usage.total_tokens
            try:
                usage = getattr(resp, "usage", None)
                if usage is not None:
                    tokens_in = int(getattr(usage, "input_tokens", 0) or 0)
                    tokens_out = int(getattr(usage, "output_tokens", 0) or 0)
                    total_tokens = int(
                        getattr(usage, "total_tokens", 0)
                        or (tokens_in + tokens_out)
                    )
            except Exception:
                # Don't break the call if usage isn't available
                pass
    
            if usage_out is not None:
                usage_out.clear()
                usage_out["model"] = model
                usage_out["tokens_in"] = tokens_in
                usage_out["tokens_out"] = tokens_out
                usage_out["total_tokens"] = total_tokens
                usage_out["cost_estimate"] = estimate_cost_usd(
                    model=model,
                    tokens_in=tokens_in,
                    tokens_out=tokens_out,
                )
    
            return text
    
        # Chat Completions path (classic 4.x / 3.5 models)
        kwargs_cc: Dict[str, object] = {
            "model": model,
            "messages": messages,
        }
        if temperature is not None:
            kwargs_cc["temperature"] = temperature
        # IMPORTANT: don't send max_tokens at all if it's None
        if max_output_tokens is not None:
            kwargs_cc["max_tokens"] = max_output_tokens
    
&gt;       resp = client.chat.completions.create(**kwargs_cc)
               ^^^^^^^^^^^
E       AttributeError: 'NoneType' object has no attribute 'chat'

C:\InfinityWindow_QA\backend\app\llm\openai_client.py:405: AttributeError</failure></testcase><testcase classname="test_api_chat_routing" name="test_chat_requires_message" time="0.311" /><testcase classname="test_api_chat_routing" name="test_chat_conversation_reuse" time="10.014"><failure message="AttributeError: 'NoneType' object has no attribute 'chat'">client = &lt;starlette.testclient.TestClient object at 0x000001D1774AF140&gt;
project = {'description': 'QA autogenerated project', 'id': 5, 'instruction_text': None, 'instruction_updated_at': None, ...}

    def test_chat_conversation_reuse(client, project):
        """C-Chat-04: Subsequent calls reuse conversation_id."""
&gt;       first = client.post(
            "/chat",
            json={"project_id": project["id"], "message": "first", "mode": "auto"},
        ).json()

C:\InfinityWindow_QA\qa\tests_api\test_api_chat_routing.py:31: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
C:\InfinityWindow_QA\.venv\Lib\site-packages\starlette\testclient.py:546: in post
    return super().post(
C:\InfinityWindow_QA\.venv\Lib\site-packages\httpx\_client.py:1144: in post
    return self.request(
C:\InfinityWindow_QA\.venv\Lib\site-packages\starlette\testclient.py:445: in request
    return super().request(
C:\InfinityWindow_QA\.venv\Lib\site-packages\httpx\_client.py:825: in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\InfinityWindow_QA\.venv\Lib\site-packages\httpx\_client.py:914: in send
    response = self._send_handling_auth(
C:\InfinityWindow_QA\.venv\Lib\site-packages\httpx\_client.py:942: in _send_handling_auth
    response = self._send_handling_redirects(
C:\InfinityWindow_QA\.venv\Lib\site-packages\httpx\_client.py:979: in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\InfinityWindow_QA\.venv\Lib\site-packages\httpx\_client.py:1014: in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\InfinityWindow_QA\.venv\Lib\site-packages\starlette\testclient.py:348: in handle_request
    raise exc
C:\InfinityWindow_QA\.venv\Lib\site-packages\starlette\testclient.py:345: in handle_request
    portal.call(self.app, scope, receive, send)
C:\InfinityWindow_QA\.venv\Lib\site-packages\anyio\from_thread.py:326: in call
    return cast(T_Retval, self.start_task_soon(func, *args).result())
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\Users\Windows 11\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\_base.py:456: in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
C:\Users\Windows 11\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\_base.py:401: in __get_result
    raise self._exception
C:\InfinityWindow_QA\.venv\Lib\site-packages\anyio\from_thread.py:257: in _call_func
    retval = await retval_or_awaitable
             ^^^^^^^^^^^^^^^^^^^^^^^^^
C:\InfinityWindow_QA\.venv\Lib\site-packages\fastapi\applications.py:1139: in __call__
    await super().__call__(scope, receive, send)
C:\InfinityWindow_QA\.venv\Lib\site-packages\starlette\applications.py:107: in __call__
    await self.middleware_stack(scope, receive, send)
C:\InfinityWindow_QA\.venv\Lib\site-packages\starlette\middleware\errors.py:186: in __call__
    raise exc
C:\InfinityWindow_QA\.venv\Lib\site-packages\starlette\middleware\errors.py:164: in __call__
    await self.app(scope, receive, _send)
C:\InfinityWindow_QA\.venv\Lib\site-packages\starlette\middleware\cors.py:85: in __call__
    await self.app(scope, receive, send)
C:\InfinityWindow_QA\.venv\Lib\site-packages\starlette\middleware\exceptions.py:63: in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
C:\InfinityWindow_QA\.venv\Lib\site-packages\starlette\_exception_handler.py:53: in wrapped_app
    raise exc
C:\InfinityWindow_QA\.venv\Lib\site-packages\starlette\_exception_handler.py:42: in wrapped_app
    await app(scope, receive, sender)
C:\InfinityWindow_QA\.venv\Lib\site-packages\fastapi\middleware\asyncexitstack.py:18: in __call__
    await self.app(scope, receive, send)
C:\InfinityWindow_QA\.venv\Lib\site-packages\starlette\routing.py:716: in __call__
    await self.middleware_stack(scope, receive, send)
C:\InfinityWindow_QA\.venv\Lib\site-packages\starlette\routing.py:736: in app
    await route.handle(scope, receive, send)
C:\InfinityWindow_QA\.venv\Lib\site-packages\starlette\routing.py:290: in handle
    await self.app(scope, receive, send)
C:\InfinityWindow_QA\.venv\Lib\site-packages\fastapi\routing.py:119: in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
C:\InfinityWindow_QA\.venv\Lib\site-packages\starlette\_exception_handler.py:53: in wrapped_app
    raise exc
C:\InfinityWindow_QA\.venv\Lib\site-packages\starlette\_exception_handler.py:42: in wrapped_app
    await app(scope, receive, sender)
C:\InfinityWindow_QA\.venv\Lib\site-packages\fastapi\routing.py:105: in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
C:\InfinityWindow_QA\.venv\Lib\site-packages\fastapi\routing.py:385: in app
    raw_response = await run_endpoint_function(
C:\InfinityWindow_QA\.venv\Lib\site-packages\fastapi\routing.py:286: in run_endpoint_function
    return await run_in_threadpool(dependant.call, **values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\InfinityWindow_QA\.venv\Lib\site-packages\starlette\concurrency.py:32: in run_in_threadpool
    return await anyio.to_thread.run_sync(func)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
C:\InfinityWindow_QA\.venv\Lib\site-packages\anyio\to_thread.py:61: in run_sync
    return await get_async_backend().run_sync_in_worker_thread(
C:\InfinityWindow_QA\.venv\Lib\site-packages\anyio\_backends\_asyncio.py:2525: in run_sync_in_worker_thread
    return await future
           ^^^^^^^^^^^^
C:\InfinityWindow_QA\.venv\Lib\site-packages\anyio\_backends\_asyncio.py:986: in run
    result = context.run(func, *args)
             ^^^^^^^^^^^^^^^^^^^^^^^^
C:\InfinityWindow_QA\backend\app\api\main.py:3526: in chat
    raw_reply_text = generate_reply_from_history(
C:\InfinityWindow_QA\backend\app\llm\openai_client.py:527: in generate_reply_from_history
    raise last_error
C:\InfinityWindow_QA\backend\app\llm\openai_client.py:505: in generate_reply_from_history
    result = _call_model(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

messages = [{'content': "You are InfinityWindow, a helpful AI assistant. You work inside a long-lived project workspace with pers...afe commands and then interpret the results you are given.\n", 'role': 'system'}, {'content': 'first', 'role': 'user'}]
model = 'gpt-4.1', temperature = 0.4, max_output_tokens = None, usage_out = {}

    def _call_model(
        messages: List[Dict[str, str]],
        model: str,
        temperature: Optional[float] = None,
        max_output_tokens: Optional[int] = None,
        usage_out: Optional[Dict[str, Any]] = None,
    ) -&gt; str:
        """
        Low‑level helper that actually calls OpenAI.
    
        - Uses Responses API for modern models (gpt‑5.*, o3.*, o4.*, gpt‑4o*).
        - Uses Chat Completions API for older chat models (gpt‑4.1, gpt‑4.1‑nano, 3.5, etc.).
        - IMPORTANT: we DO NOT send 'temperature' to the Responses API
          (models like gpt‑5‑nano reject it with a 400 error).
        """
        client = get_client()
        use_responses = _is_responses_model(model)
    
        print(
            f"[LLM] Using OpenAI model '{model}' via "
            f"{'Responses API' if use_responses else 'Chat Completions'}"
        )
    
        # We'll fill these from resp.usage when available
        tokens_in = 0
        tokens_out = 0
        total_tokens = 0
    
        if use_responses:
            # Responses API: do NOT include temperature (some models don't support it).
            kwargs: Dict[str, object] = {
                "model": model,
                "input": messages,
            }
            if max_output_tokens is not None:
                kwargs["max_output_tokens"] = max_output_tokens
    
            resp = client.responses.create(**kwargs)
            text = _extract_text_from_responses(resp)
    
            # Try to read usage.input_tokens / usage.output_tokens / usage.total_tokens
            try:
                usage = getattr(resp, "usage", None)
                if usage is not None:
                    tokens_in = int(getattr(usage, "input_tokens", 0) or 0)
                    tokens_out = int(getattr(usage, "output_tokens", 0) or 0)
                    total_tokens = int(
                        getattr(usage, "total_tokens", 0)
                        or (tokens_in + tokens_out)
                    )
            except Exception:
                # Don't break the call if usage isn't available
                pass
    
            if usage_out is not None:
                usage_out.clear()
                usage_out["model"] = model
                usage_out["tokens_in"] = tokens_in
                usage_out["tokens_out"] = tokens_out
                usage_out["total_tokens"] = total_tokens
                usage_out["cost_estimate"] = estimate_cost_usd(
                    model=model,
                    tokens_in=tokens_in,
                    tokens_out=tokens_out,
                )
    
            return text
    
        # Chat Completions path (classic 4.x / 3.5 models)
        kwargs_cc: Dict[str, object] = {
            "model": model,
            "messages": messages,
        }
        if temperature is not None:
            kwargs_cc["temperature"] = temperature
        # IMPORTANT: don't send max_tokens at all if it's None
        if max_output_tokens is not None:
            kwargs_cc["max_tokens"] = max_output_tokens
    
&gt;       resp = client.chat.completions.create(**kwargs_cc)
               ^^^^^^^^^^^
E       AttributeError: 'NoneType' object has no attribute 'chat'

C:\InfinityWindow_QA\backend\app\llm\openai_client.py:405: AttributeError</failure></testcase><testcase classname="test_search" name="test_search_requires_existing_project" time="0.318" /><testcase classname="test_api_projects" name="test_fs_path_traversal_blocked" time="0.342"><failure message="assert 422 == 400&#10; +  where 422 = &lt;Response [422 Unprocessable Entity]&gt;.status_code">client = &lt;starlette.testclient.TestClient object at 0x00000232A01DF140&gt;
project = {'description': 'QA autogenerated project', 'id': 7, 'instruction_text': None, 'instruction_updated_at': None, ...}

    def test_fs_path_traversal_blocked(client, project):
        """K-SEC-FS-01: Block traversal and UNC paths in fs endpoints."""
        pid = project["id"]
    
        bad_relative = client.get(
            f"/projects/{pid}/fs/read", params={"subpath": "..\\windows\\system32"}
        )
&gt;       assert bad_relative.status_code == 400
E       assert 422 == 400
E        +  where 422 = &lt;Response [422 Unprocessable Entity]&gt;.status_code

C:\InfinityWindow_QA\qa\tests_api\test_api_projects.py:46: AssertionError</failure></testcase><testcase classname="test_search" name="test_search_docs_validation" time="0.087"><error message="failed on setup with &quot;AssertionError: {&quot;detail&quot;:&quot;Project with that name already exists.&quot;}&#10;assert 409 == 200&#10; +  where 409 = &lt;Response [409 Conflict]&gt;.status_code&quot;">client = &lt;starlette.testclient.TestClient object at 0x0000025EC8CFEFC0&gt;

    @pytest.fixture
    def project(client: TestClient) -&gt; dict:
        resp = client.post(
            "/projects",
            json={
                "name": f"QA_Project_{os.getpid()}",
                "description": "QA autogenerated project",
                "local_root_path": LOCAL_ROOT,
            },
        )
&gt;       assert resp.status_code == 200, resp.text
E       AssertionError: {"detail":"Project with that name already exists."}
E       assert 409 == 200
E        +  where 409 = &lt;Response [409 Conflict]&gt;.status_code

C:\InfinityWindow_QA\qa\tests_api\conftest.py:71: AssertionError</error></testcase></testsuite></testsuites>