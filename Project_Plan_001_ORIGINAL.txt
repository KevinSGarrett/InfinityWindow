Part 1 – Project Plan (Full Details)
1.1 Vision & Goals

Vision:
A local, extensible “AI workbench” that:

Uses OpenAI + Anthropic (all relevant models, including multimodal).

Integrates with Slack and a ChatGPT-like web UI (e.g., LibreChat).

Has strong, persistent memory:

Can ingest huge documents (hundreds of thousands of words).

Can ingest GitHub repos and old chat logs.

Never “forgets” important details, thanks to RAG + automatic hydration.

Makes it pleasant to work on large, long-running, complex projects, avoiding the usual browser tab / context limits.

High-level goals:

ChatGPT-level UX (or better) via browser UI + Slack.

“Never forgets”:

All conversation history stored and searchable.

Docs and repos indexed so they can be referenced precisely.

Robust project memory:

Projects as first-class entities.

Per-project instructions, decisions, tasks, and docs.

Multi-provider, multi-model:

Uses OpenAI + Anthropic.

Uses text, image, audio, and (optionally) video models when needed.

Local-first:

All logs & indexes stored locally.

Easy backup/export/import.

1.2 Key Requirements (Functional)
Core chat

Slack bot:

DM interface and/or project-specific channels.

Replies with rich markdown, code blocks.

Slash commands for special features (e.g., /assistant remember, /assistant scope).

Browser UI:

ChatGPT-style interface.

Conversation list, markdown, code blocks, file upload.

Uses LibreChat (or similar) as frontend, talking to your backend via an OpenAI-compatible API.

Memory & context

Persistent storage of everything:

All messages (user + assistant).

Structured as conversations → projects.

Vectorized memory for:

Past conversation chunks.

Docs (large text files, PDFs, etc.).

GitHub code (files, functions, etc.).

Automatic hydration:

For every new message, retrieve relevant pieces from:

Past conversation.

Project docs.

GitHub repo(s).

Explicit “remembered” items.

Assemble into a compact context for the model.

Explicit “remember this”:

User can label some content as canonical memory (e.g., “our main API list”).

These are high-priority in retrieval.

Documents & large files

Ingest documents up to 600k+ words:

Use structural + hierarchical chunking:

Doc → sections → subsections → chunks.

Maintain per-document “collections” so they don’t get mixed up unintentionally.

Two-level retrieval:

First find relevant sections.

Then find relevant chunks within those sections.

Support:

Plain text.

Markdown.

PDFs (parsed to text).

(Optional later) HTML, Word, etc.

GitHub integration

Attach GitHub repos to projects.

Index:

Files (with language detection).

Directory structure.

Commit metadata (optionally).

Allow queries like:

“Explain the architecture of server/api.py.”

“Find where UserSession is instantiated.”

“Compare the config layout before and after commit X.”

Projects & knowledge organization

Projects as first-class entities:

Each conversation belongs to a project.

Each project has:

Instruction file (charter).

Decision log.

Attached docs & repos.

Tasks / TODOs.

Commands to:

Switch project.

List tasks.

Attach/onboard new repos & docs.

Multi-model & modalities

Model registry containing:

OpenAI chat models (e.g., gpt-4.x/5.x family, minis).

OpenAI image/audio models.

Anthropic Claude models (for text + images).

Router that chooses:

Which provider (OpenAI / Anthropic).

Which model (context size, cost, quality).

Which endpoint type (chat / image / audio / etc.).

Extra “assistant quality” features

Tasks & TODO extraction from conversation.

Cost/model usage dashboard.

Project export/import.

Instruction versions & snapshots.

Optional second-pass “verification” for important outputs.

Source citations (doc section, repo file path, etc.).

Topic auto-tagging of messages.

1.3 Personas & Use Cases

Primary persona: You (technical power user).

Use cases:

Long-running technical project

Multiple weeks/months.

Code in multiple repos.

Lots of design decisions & trade-offs.

Needs consistent memory of constraints, prior conclusions, and exceptions.

Research / doc-heavy work

Large PDFs / reports (hundreds of pages).

Need to query both fine-grained details and high-level summaries.

Need precise references for quotes.

Code review & refactoring

Let the system look at the entire repo.

Ask for cross-cutting changes.

Maintain memory of architecture decisions.

Knowledge management

Consolidate scattered Slack logs, notes, docs into one project-wide memory.

Ask questions like “How has our auth flow evolved over the last month?”.

1.4 Scope

In-scope (MVP + near-term):

Backend orchestrator (Python/FastAPI or similar).

SQLite/Postgres + one local vector DB (Chroma, Qdrant, LanceDB, etc.).

Slack bot integration.

LibreChat (or similar) web UI integration using OpenAI-compatible API.

Document ingestion & hierarchical indexing.

GitHub repo ingestion & indexing.

Basic model routing across OpenAI & Anthropic chat models.

RAG-based memory + automatic hydration.

“Remember this” feature (explicit long-term memory).

Task extraction & listing.

Basic cost & usage logging.

Project export.

Out-of-scope (for now / optional later):

Multi-user, multi-tenant with complex permissions.

Full CI/CD deployment pipelines.

Fine-tuning your own models.

Complex real-time collaboration edits (like Google Docs).

1.5 High-Level Architecture
Components

Frontend

Slack app:

Uses events + Web API.

For real-time chatting, quick copy/paste.

LibreChat:

Auth (local).

Talks to your orchestrator via an OpenAI-compatible /v1/chat/completions endpoint (and maybe /v1/embeddings if needed).

Orchestrator (Backend Service)

API server (FastAPI).

Responsibilities:

Receive messages from Slack & LibreChat.

Identify user, project, conversation.

Retrieve relevant memory + docs + code.

Decide which model & endpoint to call.

Call providers (OpenAI, Anthropic).

Log everything (messages, tokens, cost).

Expose endpoints for:

Document upload/ingestion.

GitHub repo operations.

Project, memory, tasks APIs.

Providers Layer

OpenAI client:

Chat models.

Embeddings.

Image generation.

Audio STT/TTS.

Anthropic client:

Chat models.

Vision (image-in).

Memory & Retrieval

Relational DB:

Users, projects, conversations, messages.

Tasks, memories, documents, repos.

Vector DB:

Chunks of:

Conversation history.

Docs & logs.

Code files.

Pinned memories.

Retrieval service:

Query by project, doc, repo, topic.

Hierarchical retrieval for big docs.

Connectors

Document ingestor:

File parsing.

Chunking.

Embeddings.

Metadata storage.

GitHub connector:

repo attach/detach.

clone / update.

index code & optionally commit metadata.

Support Services

Task extraction.

Cost logging & analytics.

Export/import.

Instruction versioning.

1.6 Phases & Milestones

Phase 0 – Foundations & Repo Setup

Decide tech stack (Python/FastAPI, Chroma, SQLite/Postgres).

Create repo structure.

Configure environment (virtualenv, Docker, etc.).

Phase 1 – Core Backend Skeleton

Basic API server.

Simple single-model OpenAI integration.

Store conversations & messages in DB.

Phase 2 – Memory Infrastructure

Add vector DB.

Implement embeddings service.

Index new messages.

Implement basic retrieval and context hydration.

Phase 3 – Slack Integration

Slack app config.

Event handler + slash commands.

Map Slack channels/threads to projects/conversations.

Phase 4 – Web UI Integration (LibreChat)

Deploy LibreChat locally.

Configure it to use your orchestrator as the OpenAI endpoint.

Test full chat flow from browser UI.

Phase 5 – Document Ingestion & Large Docs

Build ingestion pipeline for large docs.

Implement hierarchical chunking & retrieval.

UI flows for “attach documents to project”.

Phase 6 – GitHub Integration

GitHub connector & local clones.

Index repo code into vector DB.

Commands & workflows to query repo.

Phase 7 – Advanced Memory & “Never Forget”

Automatic hydration improvements.

“Remember this” feature.

Decision logs & per-project instruction files.

Phase 8 – Multi-model & Multimodal Routing

Add Anthropic integration.

Expand OpenAI to include image/audio/video tasks.

Implement routing rules and configuration.

Phase 9 – Quality-of-life Features

Task extraction & task view.

Cost dashboard.

Export/import.

Instruction versioning & snapshots.

Optional verification pass for critical outputs.

Topic auto-tagging.

Phase 10 – Hardening & Polish

Error handling & retries.

Logging & monitoring.

Security review (token storage, repo access).

Documentation.

1.7 Risks & Mitigations

Complexity sprawl

Mitigation: Strict phases, clean module boundaries, write docs as you go.

Token costs

Mitigation: Use cheap models for summarization & embeddings; careful retrieval to reduce context size; cost dashboard.

GitHub credentials misuse

Mitigation: Use read-only PAT; store encrypted; never log tokens.

Data loss

Mitigation: Regular backups of DB + vector store; export feature.

Performance issues with huge documents/repos

Mitigation: Hierarchical indexing; asynchronous ingestion; caching of section summaries.

1.8 Non-Functional Requirements

Performance:

Single request < a few seconds in normal use.

Large operations (ingest, reindex) can run asynchronously.

Security & Privacy:

All user data stored locally.

API keys in environment variables or encrypted config.

Proper access control for Slack & web.

Maintainability:

Clear modular design.

Config-driven model registry.

Logs and metrics for debugging.

Extensibility:

Easy to add:

New model providers.

New connectors (e.g., Notion, Google Drive).

New tools/agents.

Part 2 – System Outline (Everything to Build)

This is the technical outline: modules, components, and their responsibilities.

2.1 Repos & Top-Level Structure

Assume one main monorepo:

ai-workbench/
  backend/
    src/
    tests/
  ui-libredirect/        # LibreChat config/customization (optional)
  infra/
    docker-compose.yml
    k8s/ (optional later)
  docs/
    architecture.md
    api.md
    operations.md
  scripts/
    ingest_docs.py
    ingest_logs.py
    index_repo.py


You can adjust names, but this is the idea.

2.2 Backend Modules (Detailed)
2.2.1 API Server

Folder: backend/src/api/

main.py

FastAPI app.

Routers:

/chat – internal API for Slack (send message, get reply).

/openai/v1/chat/completions – OpenAI-compatible endpoint for LibreChat.

/docs – document uploading, listing.

/projects – CRUD for projects.

/github – attach repo, sync status, search.

/tasks – list/update tasks.

/export – export project data.

Middleware:

Auth for web UI.

Logging.

Rate limiting (if needed).

2.2.2 Orchestrator

Folder: backend/src/orchestrator/

router.py

Takes an internal ChatRequest:

user_id, project_id, conversation_id, message.

Calls:

Context hydration.

Model routing.

Provider invocation.

Returns ChatResponse.

context_hydration.py

Given a new message:

Fetch recent conversation messages.

Retrieve relevant memory chunks:

Past messages.

Docs.

GitHub code.

Pinned memories.

Combine with project instructions & decision logs.

Build final system + user + assistant message list.

tool_invocation.py

Defines internal tools:

search_docs()

search_code()

remember()

create_task()

etc.

If using tool-calling models, orchestrates the tool-calling loop.

model_router.py

Loads model registry config.

Chooses:

Provider (OpenAI/Anthropic).

Model ID.

Endpoint type (chat, image, audio).

Based on:

Task type (chat, code, summarization, generation).

Context size.

Cost/quality preferences.

2.2.3 Providers Layer

Folder: backend/src/providers/

openai_client.py

Wrapper for:

Chat completions.

Embeddings.

Image generation.

Audio STT/TTS.

anthropic_client.py

Wrapper for:

Chat completions.

Vision input.

embeddings.py

Unified interface for embeddings (probably OpenAI text embeddings).

Can swap to local embeddings later if desired.

2.2.4 Memory & Retrieval

Folder: backend/src/memory/

db.py

SQLAlchemy models for:

User

Project

Conversation

Message

Document

DocumentSection

DocumentChunk

Repo

RepoFile

MemoryItem (for “remember this”)

Task

InstructionSnapshot

UsageRecord (tokens/cost)

vector_store.py

Abstraction over Chroma/Qdrant/etc.

Methods:

index_chunks(chunks)

search_chunks(project_id, query, filters, top_k)

search_doc_chunks(document_id, query, top_k)

search_code_chunks(repo_id, query, top_k)

search_pinned_memories(project_id, query, top_k)

retrieval.py

Higher-level logic, orchestrating hierarchical retrieval:

Stage 1: section-level.

Stage 2: chunk-level.

2.2.5 Ingestors

Folder: backend/src/ingestion/

docs_ingestor.py

For each file:

Parse text.

Split into sections if possible (headings / structure).

Chunk.

Embed.

Store metadata in DB + vector store.

logs_ingestor.py

Import old chat logs into:

Messages table (with proper projects and conversations).

Vector store for retrieval.

github_ingestor.py

Clone/update repos.

Walk through files:

Filter by type.

Chunk code.

Embed & store into vector store.

2.2.6 Extras

Folder: backend/src/features/

remember.py

Endpoints + logic for pinned memories:

Create/update/list.

tasks.py

Task extraction logic (using small model).

CRUD for tasks table.

analytics.py

Usage logging.

Cost calculations.

Aggregation queries.

exports.py

Package project data into zip/tarball.

Import logic.

2.3 Slack Integration

Folder: backend/src/integrations/slack/

Slack app using Bolt (or just HTTP handlers):

events.py:

Handle message events.

Map Slack user/channel/thread → internal user/project/conversation.

commands.py:

/assistant remember

/assistant scope

/assistant tasks

formatting.py:

Convert internal markdown → Slack mrkdwn.

Build block kit structures.

2.4 Web UI Integration (LibreChat)

Folder: ui-libredirect/ or docs & config in docs/

LibreChat config:

Set API base URL to your orchestrator’s OpenAI-compatible endpoint.

Configure default model list, mapping to your internal models.

Optionally customize:

Logo/name.

Preset prompts.

File upload behavior (pointing to your /docs endpoints).

You won’t modify LibreChat code heavily; you’ll mostly configure it.

2.5 Config & Operations

Folder: backend/src/config/

settings.py

Read env vars / config file.

Contains:

Provider API keys.

DB connection strings.

Vector DB config.

Model registry.

models.yml

Define the set of models and their attributes as described earlier.

Infra:

infra/docker-compose.yml:

Services:

backend

vector_db (e.g., Chroma)

db (Postgres/SQLite via volume)

librechat (optional)

Setup for logs, volumes, and networking between services.

Part 3 – Detailed To‑Do List (Structured Checklist)

You can paste this into your task manager or keep it as a markdown doc.

Phase 0 – Foundations & Environment

Repo & environment

 Create ai-workbench git repository.

 Initialize backend folder with Python project structure.

 Decide on framework (e.g., FastAPI) and add base dependencies:

 fastapi

 uvicorn

 httpx (for calling LLM APIs)

 pydantic

 SQLAlchemy + DB driver

 Chroma/Qdrant client

 Create virtual environment & requirements file (or Poetry/UV config).

 Set up basic .gitignore (Python, OS files, etc.).

Database & vector store

 Choose relational DB (SQLite to start, optional Postgres later).

 Choose vector DB (e.g., Chroma for local).

 Add docker-compose with:

 relational DB container (if not SQLite).

 vector DB container (if needed).

Config & secrets

 Define .env file template with:

 OPENAI_API_KEY

 ANTHROPIC_API_KEY

 GITHUB_TOKEN

 SLACK_BOT_TOKEN

 SLACK_SIGNING_SECRET

 Implement config loader (settings.py).

Phase 1 – Core Backend Skeleton

API server

 Create backend/src/api/main.py with FastAPI app.

 Add health check endpoint: GET /health.

 Implement simple POST /chat/test that echoes input.

Data models (DB)

 Define User model/table.

 Define Project model/table.

 Define Conversation model/table.

 Define Message model/table.

 Create Alembic or migration scripts (if using SQLAlchemy).

Basic chat flow (single model)

 Implement providers/openai_client.py with:

 chat_completion(messages, model_id, **kwargs).

 Implement simple “no memory” chat endpoint:

 POST /chat/basic:

Accepts conversation ID + message.

Calls OpenAI model (e.g., gpt-4.x).

Returns result.

 Store:

 Incoming user messages in DB.

 Assistant responses in DB.

Phase 2 – Memory Infrastructure & Retrieval

Message indexing

 Add embedding support in providers/embeddings.py.

 Add vector store abstraction memory/vector_store.py with:

 add_message_embeddings(messages)

 search_messages(project_id, query, top_k)

Document models

 Add Document model/table.

 Add DocumentSection model/table.

 Add DocumentChunk model/table.

Retrieval service

 Implement memory/retrieval.py:

 Function to retrieve relevant conversation chunks by query.

 Function to retrieve relevant doc chunks by query and project.

 Implement rough “context builder”:

 Given a new user message:

Fetch last N messages.

Fetch top K relevant past messages.

Return combined list.

Integrate into chat flow

 Replace basic POST /chat/basic with POST /chat that:

 Identifies project/conversation.

 Performs retrieval.

 Builds prompt & calls OpenAI.

 Stores new message + embeddings.

Phase 3 – Slack Integration

Slack app setup

 Create Slack app in Slack UI.

 Enable:

 Bot user.

 Event Subscriptions (message events).

 Slash commands.

 Configure redirect/URL to your backend (e.g., /slack/events).

 Install app into your workspace; grab bot token & signing secret.

Backend integration

 Add integrations/slack/events.py:

 Verify Slack signature.

 Receive events.

 For message events:

 Map Slack user to internal user.

 Map channel/thread to project/conversation.

 Call orchestrator with message.

 Add integrations/slack/commands.py:

 Implement /assistant command:

 Subcommands for remember, scope, tasks, etc.

 Add Slack formatting helper:

 Convert markdown to Slack mrkdwn.

 Wrap responses in block kit sections.

Test Slack workflow

 Send DM to bot:

 Confirm message flows through orchestrator and back.

 Test thread-based conversation in a channel.

Phase 4 – Web UI Integration (LibreChat)

LibreChat setup

 Set up LibreChat via Docker or manual install.

 Confirm it works against a dummy OpenAI key (or mock).

Orchestrator OpenAI-compatible endpoint

 Implement POST /openai/v1/chat/completions:

 Accept OpenAI-style payload.

 Convert to internal ChatRequest.

 Call orchestrator.

 Return OpenAI-style response JSON.

LibreChat config

 Configure LibreChat to use your custom API base URL.

 Define available model names (mapped to your internal model IDs).

 Test:

 Chat in LibreChat uses your backend.

 Messages appear in your DB.

Phase 5 – Document Ingestion & Large Docs

Parsing & chunking

 Implement ingestion/docs_ingestor.py:

 Accept file upload (or path).

 Parse plain text.

 Add logic to detect headings/sections (e.g., markdown headings, PDF sections).

 Create Document record.

 Create DocumentSection records.

 Chunk sections into ~1–2k token DocumentChunks with overlaps.

 Implement embeddings for DocumentChunks and store them in vector DB with metadata:

 project_id

 document_id

 section_path

 chunk_index

Hierarchical retrieval

 In retrieval.py:

 Implement section-level search (using section summaries or first chunk).

 Implement chunk-level search restricted to top sections.

Project association & API

 Extend /docs endpoints:

 POST /docs/upload:

 Attach document to project.

 Trigger ingestion.

 GET /projects/{id}/docs:

 List docs for project.

Testing large docs

 Create or use a synthetic 600k-word text.

 Ingest and ensure:

 Performance is acceptable.

 Retrieval pulls only relevant sections.

Phase 6 – GitHub Integration

Repo metadata & models

 Add Repo model/table:

 project_id

 provider (e.g., github)

 repo_full_name

 local_path

 default_branch

 last_indexed_commit

 Add RepoFile model/table:

 repo_id

 path

 language

 last_indexed_sha

Connector

 Implement ingestion/github_ingestor.py:

 Clone repo with PAT.

 Pull latest changes on subsequent runs.

 Walk through files:

 Apply .gitignore.

 Filter by file extensions (e.g., code + docs).

 Chunk files (e.g., by function or by lines).

 Store RepoFile records.

 Embed chunks and index in vector DB.

API & commands

 Add /github/attach endpoint:

 Attach repo_full_name to project and start indexing.

 Add Slack command /assistant repo attach myuser/myrepo.

 Implement retrieval calls:

 search_code_chunks(repo_id, query, top_k).

 Example workflows:

 Ask Slack bot/LibreChat: “Explain server/api.py in my attached repo”.

 Ask: “Find all the usages of UserSession”.

Phase 7 – Advanced Memory & “Never Forget”

Pinned memories (“remember this”)

 Add MemoryItem model/table:

 project_id

 key (e.g., canonical_auth_flow)

 content

 tags

 Implement features/remember.py:

 Create/update/list memory items.

 Embed memory items into dedicated vector collection.

 Slack + Chat UX:

 Pattern like: “Remember this as <key>”.

 Slash command: /assistant remember key=canonical_auth_flow.

 Hydration:

 Always retrieve pinned memories for the project that match query/topic and prepend them to context.

Project instructions & decision log

 Add fields to Project:

 instruction_text

 decision_log (or separate table).

 Provide endpoints/UI to:

 View/edit instructions.

 Append decisions.

 Always inject instructions + key decisions into system prompt for the project.

Automatic hydration tuning

 Implement scoring/ranking for retrieved chunks:

 De-duplication.

 Diversity (not all from a single spot).

 Limit total tokens from memory.

 Optionally use a small model to compress or summarize long retrieved context before final call.

Phase 8 – Multi-Model & Multimodal Routing

Model registry

 Create config/models.yml to define:

 All OpenAI chat models (with context sizes, cost class, quality).

 All relevant Anthropic models.

 OpenAI image/audio models.

 Implement model_router.py to:

 Load this registry.

 Filter by:

 type: chat/image/audio.

 context_tokens vs estimated context size.

 Cost tier.

 Quality flag.

 Choose a model based on “task classification”.

Task classification

 Implement pre-routing detection of task type:

 Chat/explanation vs code vs summarization vs generation (image/audio).

 For prompts like:

“Generate an image of…” → route to image model.

“Transcribe this audio…” → route to audio STT, then to chat for summary.

Provider integration

 Expand openai_client.py:

 Implement image generation method.

 Implement audio transcription.

 Implement audio TTS (if used).

 Expand anthropic_client.py to include all desired models & options.

Verification pass (optional)

 Implement option:

After main response, call a cheaper model with “critique/verify this answer” prompt.

Attach notes to the final response or internal log.

Phase 9 – Quality-of-Life Features

Task extraction

 Add Task model/table:

 project_id

 description

 status (open/closed)

 created_from_message_id

 due_date (optional)

 Implement features/tasks.py:

 Small-model run after each message:

Detect TODOs.

Add tasks automatically.

 Endpoints for:

Listing tasks by project.

Updating status.

 Slack + UI:

 /assistant tasks to list.

 Option to mark done.

Cost & usage dashboard

 Add UsageRecord model/table:

 project_id

 model

 tokens_in

 tokens_out

 cost_estimate

 timestamp

 Log a UsageRecord for each provider call.

 Implement /analytics/usage endpoint.

 Simple web dashboard (can be within backend or LibreChat custom page) to show:

Tokens & cost per project.

Model usage distribution.

Export/import

 Implement exports.py:

 export_project(project_id):

Dump DB rows for:

Project, conversations, messages, documents, tasks, memories, instructions.

Optionally export vector data or just references.

Zip them.

 import_project(zip):

Recreate DB entities.

 Add CLI command or API for export/import.

Instruction versioning & snapshots

 Add InstructionSnapshot model:

 project_id

 version

 content

 created_at

 Whenever project instructions change:

 Save a snapshot.

 Allow:

 Viewing history.

 Comparing versions (LLM can summarize differences).

Topic auto-tagging

 Add topics JSON field to Message.

 After each message, call small model to tag with topics (e.g., ["auth", "frontend", "infra"]).

 Use topics as filters in retrieval.

Slack “scope” controls

 Implement /assistant scope with subcommands:

 project <name>

 doc <document_id>

 repo <repo_name>

 Store current scope in conversation metadata.

 Retrieval layer uses scope to constrain target corpora.

Phase 10 – Hardening & Polish

Error handling

 Wrap provider calls with:

 Retry on transient errors.

 Timeouts.

 Clear error messages.

 Gracefully handle:

 Exceeded token limits.

 Missing project or repo mappings.

Logging & observability

 Structured logging (JSON).

 Log:

 Request ID.

 User/project/conversation.

 Provider/model.

 Latency.

 Optional: integrate with a simple log viewer or visualization.

Security

 Review how API keys are stored and loaded.

 Ensure tokens aren’t logged.

 Validate Slack signatures.

 Protect backend endpoints (especially export/import) with auth.

Documentation

 docs/architecture.md:

 Overview, diagrams.

 docs/api.md:

 Paths, payloads, examples.

 docs/operations.md:

 How to run, backup, restore, upgrade.

 README.md with:

 Quick start.

 Configuration instructions.

 Known limitations.